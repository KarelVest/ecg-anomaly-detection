{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from math import pi\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "filePath = filedialog.askopenfilename(filetypes=[(\"Parquet Files\", \"*.parquet\")])\n",
    "if filePath:\n",
    "    print(\"Выбранный файл:\", filePath)\n",
    "    fileName, fileExtension = os.path.splitext(filePath)\n",
    "    if fileExtension == '.parquet':\n",
    "        dfOriginal = pd.read_parquet(filePath)\n",
    "        df = dfOriginal.copy()\n",
    "    else:\n",
    "        print(\"Выбран неверный файл\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Выбор файла отменен.\")\n",
    "    exit()\n",
    "\n",
    "df['value'] = (df['value'] - df['value'].min()) / (df['value'].max() - df['value'].min())       # Нормализация данных\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondEcgDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.segmentStarts = df.index[df['edge'] > 0].tolist()\n",
    "        self.startsCount = len(self.segmentStarts)\n",
    "\n",
    "        self.cachedTensors = []\n",
    "        self._get_segment_tensor()\n",
    "        self.segmentsCount = len(self.cachedTensors)\n",
    "    \n",
    "    def _get_segment_tensor(self):\n",
    "        for index in range(self.startsCount):\n",
    "            result = 0\n",
    "            if index + 1 < len(self.segmentStarts):\n",
    "                if self.df.at[self.segmentStarts[index], 'edge'] < 2:\n",
    "                    segment = self.df.iloc[self.segmentStarts[index]:self.segmentStarts[index+1]]\n",
    "                    result = 1\n",
    "            elif self.df.at[self.segmentStarts[index], 'edge'] < 2:\n",
    "                segment = self.df.iloc[self.segmentStarts[index]:]\n",
    "                result = 1\n",
    "\n",
    "            if result:\n",
    "                segmentValues = segment['value']\n",
    "                segmentTensor = torch.tensor(segmentValues.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "                padding = 200 - len(segmentTensor)\n",
    "                if padding > 0:\n",
    "                    segmentTensor = torch.cat((segmentTensor, torch.zeros(padding, 1)), dim=0)\n",
    "                elif padding < 0:\n",
    "                    print('В данные зашёл сегмент неверной длины')\n",
    "                self.cachedTensors.append(segmentTensor)\n",
    "\n",
    "    # Часто вызываемые во врмемя вычислений функции (оптимизированы)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.segmentsCount\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        segmentTensor = self.cachedTensors[index]\n",
    "        return segmentTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SecondEcgDataset(df)\n",
    "print(len(dataset))\n",
    "trainDataset, valDataset = random_split(dataset, [int(0.8 * len(dataset)), len(dataset) - int(0.8 * len(dataset))])\n",
    "print(len(trainDataset))\n",
    "print(len(valDataset))\n",
    "\n",
    "trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=10, shuffle=True)\n",
    "valDataloader = torch.utils.data.DataLoader(valDataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=64,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=1,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        # Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        output, _ = self.encoder(x)\n",
    "\n",
    "        # Decoder\n",
    "        output, _ = self.decoder(output)\n",
    "\n",
    "        # Sigmoid\n",
    "        output = self.sigmoid(output)       # Shape torch.Size([10, 200, 1])\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры для создания модели\n",
    "input_size = 1 # Размерность сигнала\n",
    "hidden_size = 64 # Кол-во нейронов в слое\n",
    "num_layers = 1 # Кол-во своёв\n",
    "\n",
    "# Создание экземпляра модели\n",
    "device = (\"cuda\")\n",
    "# model = LSTMAutoencoder(input_size, hidden_size, num_layers).to(device)\n",
    "model = LSTMAutoencoder().to(device)\n",
    "print(model)\n",
    "# Определение функции потерь и оптимизатора\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "def train(trainDataloader, valDataloader, model, criterion, optimizer):\n",
    "    # Обучение модели\n",
    "    model.train()\n",
    "    trainPbar = tqdm(trainDataloader, desc=\"Training\")\n",
    "    for inputs in trainPbar:\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        losses = criterion(outputs, inputs)\n",
    "        # loss.backward()\n",
    "        for loss in losses:\n",
    "            averLoss = torch.mean(loss)\n",
    "            averLoss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        #trainPbar.set_postfix({'Loss': f'{losses[-1].item()}'})\n",
    "\n",
    "    # Валидация модели\n",
    "    model.eval() # Переводим модель в режим валидации\n",
    "    valPbar = tqdm(valDataloader, desc=\"Validating\")\n",
    "    with torch.no_grad(): # Отключаем вычисление градиентов\n",
    "        for inputs in valPbar:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            valLosses = criterion(outputs, inputs)\n",
    "            for vloss in valLosses:\n",
    "                averVLoss = torch.mean(vloss)\n",
    "            # valPbar.set_postfix({'Val_Loss': f'{val_loss.item()}'})\n",
    "    \n",
    "    trainPbar.close()\n",
    "    valPbar.close()\n",
    "\n",
    "    return averLoss.item(), averVLoss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll, vl = 0, 0\n",
    "for epoch in range(3000):\n",
    "    print(f'Epoch: {epoch} ')\n",
    "    ll, vl = train(trainDataloader, valDataloader, model, criterion, optimizer)\n",
    "\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss: ', ll)\n",
    "print('ValLoss: ', vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), '../00. Resources/Models/SecondStageModel.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
