{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_52017/2001088470.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Импорт библиотек\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from math import pi\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранный файл: /home/vest/Desktop/Work/NeuralNetworks/ecg-anomaly-detection/.workspace/Resources/SecondDone/ECGr1.C.S.V.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>edge</th>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.194514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608595.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608596.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608597.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608598</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608598.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608599.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           value  edge      time  group\n",
       "0       0.190297   0.0       0.0    NaN\n",
       "1       0.164994   0.0       1.0    NaN\n",
       "2       0.177645   0.0       2.0    NaN\n",
       "3       0.194514   0.0       3.0    NaN\n",
       "4       0.160777   0.0       4.0    NaN\n",
       "...          ...   ...       ...    ...\n",
       "608595  0.000000   0.0  608595.0    NaN\n",
       "608596  0.000000   0.0  608596.0    NaN\n",
       "608597  0.000000   0.0  608597.0    NaN\n",
       "608598  0.000000   0.0  608598.0    NaN\n",
       "608599  0.000000   0.0  608599.0    NaN\n",
       "\n",
       "[608600 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# #! Сначала склеить разметку, а потом уже определить разрывы и разметить цифрой 2 начала предупреждающих участков\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "filePath = filedialog.askopenfilename(filetypes=[(\"Parquet Files\", \"*.parquet\")])\n",
    "if filePath:\n",
    "    print(\"Выбранный файл:\", filePath)\n",
    "    fileName, fileExtension = os.path.splitext(filePath)\n",
    "    if fileExtension == '.parquet':\n",
    "        dfOriginal = pd.read_parquet(filePath)\n",
    "        df = dfOriginal.copy()\n",
    "    else:\n",
    "        print(\"Выбран неверный файл\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Выбор файла отменен.\")\n",
    "    exit()\n",
    "\n",
    "df['value'] = (df['value'] - df['value'].min()) / (df['value'].max() - df['value'].min())       # Нормализация данных\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SecondEcgDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "#         self.segmentStarts = df.index[df['edge'] > 0].tolist()\n",
    "#         self.startsCount = len(self.segmentStarts)\n",
    "\n",
    "#         self.cachedTensors = []\n",
    "#         self._get_segment_tensor()\n",
    "#         self.segmentsCount = len(self.cachedTensors)\n",
    "    \n",
    "#     def _get_segment_tensor(self):\n",
    "#         for index in range(self.startsCount):\n",
    "#             result = 0\n",
    "#             if index + 1 < len(self.segmentStarts):\n",
    "#                 if self.df.at[self.segmentStarts[index], 'edge'] < 2:\n",
    "#                     segment = self.df.iloc[self.segmentStarts[index]:self.segmentStarts[index+1]]\n",
    "#                     result = 1\n",
    "#             elif self.df.at[self.segmentStarts[index], 'edge'] < 2:\n",
    "#                 segment = self.df.iloc[self.segmentStarts[index]:]\n",
    "#                 result = 1\n",
    "\n",
    "#             if result:\n",
    "#                 segmentValues = segment['value']\n",
    "#                 segmentTensor = torch.tensor(segmentValues.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "#                 padding = 200 - len(segmentTensor)\n",
    "#                 if padding > 0:\n",
    "#                     segmentTensor = torch.cat((segmentTensor, torch.zeros(padding, 1)), dim=0)\n",
    "#                 elif padding < 0:\n",
    "#                     print('В данные зашёл сегмент неверной длины')\n",
    "#                 self.cachedTensors.append(segmentTensor)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self.segmentsCount\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         segmentTensor = self.cachedTensors[index]\n",
    "#         return segmentTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondEcgDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.segmentStarts = df.index[df['edge'] > 0].tolist()\n",
    "        self.startsCount = len(self.segmentStarts)\n",
    "\n",
    "        self.cachedTensors = []\n",
    "        self.cachedLabels = []\n",
    "        self._get_segment_tensor()\n",
    "        self.segmentsCount = len(self.cachedTensors)\n",
    "    \n",
    "    def _get_segment_tensor(self):\n",
    "        for index in range(self.startsCount):\n",
    "            result = 0\n",
    "            if index + 1 < len(self.segmentStarts):\n",
    "                if self.df.at[self.segmentStarts[index], 'edge'] != 2:      # По сути условие, что длина отрезка меньше 200\n",
    "                    segment = self.df.iloc[self.segmentStarts[index]:self.segmentStarts[index+1]]\n",
    "                    result = 1\n",
    "            elif self.df.at[self.segmentStarts[index], 'edge'] != 2:        #! Возможно это нужно будет убрать, т.к. тут в выборку может попасть отрезок длиной больше 200\n",
    "                segment = self.df.iloc[self.segmentStarts[index]:]\n",
    "                result = 1\n",
    "\n",
    "            if result:\n",
    "                segmentValues = segment['value']\n",
    "                labelValue = (1 if segment.iloc[0]['edge'] == 3 else 0)\n",
    "                segmentTensor = torch.tensor(segmentValues.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "                # print(segmentTensor.shape)\n",
    "                labelTensor = torch.tensor(labelValue, dtype=torch.float).unsqueeze(0)\n",
    "                # print(labelTensor.shape)\n",
    "                padding = 200 - len(segmentTensor)\n",
    "                if padding > 0:\n",
    "                    segmentTensor = torch.cat((segmentTensor, torch.zeros(padding, 1)), dim=0)\n",
    "                elif padding < 0:\n",
    "                    print('В данные зашёл сегмент неверной длины')\n",
    "                    exit()\n",
    "                self.cachedTensors.append(segmentTensor)\n",
    "                self.cachedLabels.append(labelTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.segmentsCount\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #segmentTensor = self.cachedTensors[index]\n",
    "        #return segmentTensor\n",
    "        return self.cachedTensors[index], self.cachedLabels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5505\n",
      "3853\n",
      "1652\n"
     ]
    }
   ],
   "source": [
    "dataset = SecondEcgDataset(df)\n",
    "print(len(dataset))\n",
    "#! Подумать, как размешать датасет перед разделением\n",
    "trainDataset, valDataset = random_split(dataset, [int(0.7 * len(dataset)), len(dataset) - int(0.7 * len(dataset))])\n",
    "# trainDataset, valDataset, testDataset = random_split(dataset, [int(0.6 * len(dataset)), int(0.2 * len(dataset)), len(dataset) - int(0.6 * len(dataset)) - int(0.2 * len(dataset))])\n",
    "print(len(trainDataset))\n",
    "print(len(valDataset))\n",
    "\n",
    "trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=10, shuffle=True)\n",
    "valDataloader = torch.utils.data.DataLoader(valDataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMAutoencoder(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, num_layers=1, bidirectional=True):\n",
    "#         super(LSTMAutoencoder, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder = nn.LSTM(\n",
    "#             input_size=(2 * hidden_size if bidirectional else hidden_size),\n",
    "#             hidden_size=input_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         self.resulter = nn.Linear(2 * input_size if bidirectional else input_size, 1)\n",
    "\n",
    "#         # Sigmoid\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         output, _ = self.encoder(x)\n",
    "\n",
    "#         # Decoder\n",
    "#         output, _ = self.decoder(output)\n",
    "\n",
    "#         # Resulter\n",
    "#         output = self.resulter(output)\n",
    "\n",
    "#         # Sigmoid\n",
    "#         output = self.sigmoid(output) \n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMAutoencoder(nn.Module):\n",
    "#     def __init__(self, input_size=1, hidden_size=64, num_layers=1, bidirectional=True):\n",
    "#         super(LSTMAutoencoder, self).__init__()\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder1 = nn.LSTM(\n",
    "#             input_size=input_size,\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         self.encoder2 = nn.LSTM(\n",
    "#             input_size=(2*hidden_size if bidirectional else hidden_size),\n",
    "#             hidden_size=hidden_size//2,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         self.encoder3 = nn.LSTM(\n",
    "#             input_size=(hidden_size if bidirectional else hidden_size//2),\n",
    "#             hidden_size=hidden_size//4,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         # Decoder\n",
    "#         self.decoder1 = nn.LSTM(\n",
    "#             input_size=(hidden_size//2 if bidirectional else hidden_size//4),\n",
    "#             hidden_size=hidden_size//2,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         self.decoder2 = nn.LSTM(\n",
    "#             input_size=(hidden_size if bidirectional else hidden_size//2),\n",
    "#             hidden_size=hidden_size,\n",
    "#             num_layers=num_layers,\n",
    "#             batch_first=True,\n",
    "#             bidirectional=bidirectional\n",
    "#         )\n",
    "\n",
    "#         self.resulter = nn.Linear(2 * hidden_size if bidirectional else hidden_size, 1)\n",
    "\n",
    "#         # Sigmoid\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Encoder\n",
    "#         output, _ = self.encoder1(x)\n",
    "#         output, _ = self.encoder2(output)\n",
    "#         output, _ = self.encoder3(output)\n",
    "\n",
    "#         # Decoder\n",
    "#         output, _ = self.decoder1(output)\n",
    "#         output, _ = self.decoder2(output)\n",
    "\n",
    "#         # Resulter\n",
    "#         output = self.resulter(output)\n",
    "\n",
    "#         # Sigmoid\n",
    "#         output = self.sigmoid(output) \n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, bidirectional=True):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        self.resulter = nn.Linear(2 * hidden_size if bidirectional else hidden_size, 1)\n",
    "\n",
    "        # Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2 if self.bidirectional else self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Encoder\n",
    "        _, (hidden, _) = self.lstm(x, (h0, c0))\n",
    "        # print(torch.cat([hidden[-2], hidden[-1]], dim=1).shape)\n",
    "\n",
    "        # Resulter\n",
    "        output = self.resulter(torch.cat([hidden[-2], hidden[-1]], dim=1))\n",
    "        # print(output.shape)\n",
    "\n",
    "        # Sigmoid\n",
    "        output = self.sigmoid(output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (lstm): LSTM(1, 64, batch_first=True, bidirectional=True)\n",
      "  (resulter): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Параметры для создания модели\n",
    "input_size = 1 # Так как ЭКГ - одномерный сигнал\n",
    "hidden_size = 64 # Можно изменить в зависимости от сложности задачи и размера данных\n",
    "num_layers = 1 # Можно изменить в зависимости от сложности задачи\n",
    "\n",
    "# Создание экземпляра модели\n",
    "device = (\"cuda\")\n",
    "# model = LSTMAutoencoder(input_size, hidden_size, num_layers).to(device)\n",
    "model = LSTMAutoencoder().to(device)\n",
    "print(model)\n",
    "# Определение функции потерь и оптимизатора\n",
    "# criterion = nn.MSELoss()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "def train(trainDataloader, valDataloader, model, criterion, optimizer):\n",
    "    # Обучение модели\n",
    "    model.train()\n",
    "    trainPbar = tqdm(trainDataloader, desc=\"Training\")\n",
    "    for inputs, targets in trainPbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainPbar.set_postfix({'Loss': f'{loss.item()}'})\n",
    "\n",
    "    # Валидация модели\n",
    "    model.eval() # Переводим модель в режим валидации\n",
    "    valPbar = tqdm(valDataloader, desc=\"Validating\")\n",
    "    with torch.no_grad(): # Отключаем вычисление градиентов\n",
    "        for inputs, targets in valPbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            valLoss = criterion(outputs, targets)\n",
    "            valPbar.set_postfix({'Val_Loss': f'{valLoss.item()}'})\n",
    "    \n",
    "    trainPbar.close()\n",
    "    valPbar.close()\n",
    "\n",
    "    return loss.item(), valLoss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 346.44it/s, Loss=0.6443926095962524]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 887.76it/s, Val_Loss=0.6430180072784424]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 399.05it/s, Loss=0.5798404216766357]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 820.22it/s, Val_Loss=0.5788750648498535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 396.67it/s, Loss=0.23759737610816956]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 819.99it/s, Val_Loss=0.23607875406742096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 375.08it/s, Loss=0.043512389063835144]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 780.58it/s, Val_Loss=0.04330586642026901] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 405.28it/s, Loss=0.03409041091799736] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 842.05it/s, Val_Loss=0.034043800085783005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 395.52it/s, Loss=1.1831862926483154]  \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 822.84it/s, Val_Loss=0.030952095985412598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 374.17it/s, Loss=0.029986433684825897]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 797.78it/s, Val_Loss=0.029965654015541077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 338.10it/s, Loss=0.029153190553188324]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 784.38it/s, Val_Loss=0.029051262885332108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 390.86it/s, Loss=0.028941093012690544]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 803.18it/s, Val_Loss=0.028841976076364517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 363.43it/s, Loss=0.027863234281539917]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 814.40it/s, Val_Loss=0.027818458154797554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 395.33it/s, Loss=0.027987953275442123]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 803.27it/s, Val_Loss=0.02793622761964798] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 379.16it/s, Loss=0.027754127979278564]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 781.91it/s, Val_Loss=0.027764949947595596]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 352.33it/s, Loss=0.028217986226081848]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 795.65it/s, Val_Loss=0.028153572231531143]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 388.55it/s, Loss=0.02814417891204357] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 798.71it/s, Val_Loss=0.028096923604607582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 379.61it/s, Loss=0.028070492669939995]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 776.55it/s, Val_Loss=0.028047019615769386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 359.11it/s, Loss=0.028127171099185944]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 817.17it/s, Val_Loss=0.028151176869869232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 395.78it/s, Loss=0.028610166162252426]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 794.80it/s, Val_Loss=0.028570473194122314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 404.11it/s, Loss=0.02800658345222473] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 840.99it/s, Val_Loss=0.02795501798391342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 381.87it/s, Loss=0.02838335931301117] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 822.89it/s, Val_Loss=0.028279734775424004]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 351.76it/s, Loss=0.028059273958206177]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 786.96it/s, Val_Loss=0.02802451141178608] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 381.05it/s, Loss=0.028044655919075012]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 836.43it/s, Val_Loss=0.02795873023569584]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 391.55it/s, Loss=0.028411265462636948]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 803.30it/s, Val_Loss=0.02834528125822544] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 281.36it/s, Loss=0.02837473526597023] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 534.43it/s, Val_Loss=0.02829178050160408] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 301.51it/s, Loss=0.02855342999100685] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 818.69it/s, Val_Loss=0.028471555560827255]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 404.46it/s, Loss=0.028469350188970566]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 824.99it/s, Val_Loss=0.028399022296071053]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 298.86it/s, Loss=0.0282018780708313]  \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 760.80it/s, Val_Loss=0.028179679065942764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 373.74it/s, Loss=0.02888934314250946] \n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 773.71it/s, Val_Loss=0.028833992779254913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 362.76it/s, Loss=0.027962079271674156]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 743.07it/s, Val_Loss=0.027959125116467476]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:01<00:00, 327.75it/s, Loss=0.028193941339850426]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 748.11it/s, Val_Loss=0.028164170682430267]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 386/386 [00:00<00:00, 394.12it/s, Loss=0.028239702805876732]\n",
      "Validating: 100%|██████████| 166/166 [00:00<00:00, 833.60it/s, Val_Loss=0.02822176180779934]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ll, vl = 0, 0\n",
    "for epoch in range(30):\n",
    "    print(f'Epoch: {epoch} ')\n",
    "    ll, vl = train(trainDataloader, valDataloader, model, criterion, optimizer)\n",
    "\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.028239702805876732\n",
      "ValLoss:  0.02822176180779934\n"
     ]
    }
   ],
   "source": [
    "print('Loss: ', ll)\n",
    "print('ValLoss: ', vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), '../.workspace/Models/SecondStageModel.25.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
