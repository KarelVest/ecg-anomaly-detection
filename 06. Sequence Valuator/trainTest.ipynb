{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "from math import pi\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Выбранный файл: /home/vest/Desktop/Work/NeuralNetworks/ecg-anomaly-detection/.workspace/Resources/05. Valuated/ECGr1perfect.C.S.V.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>edge</th>\n",
       "      <th>time</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.164994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.177645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.194514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608595</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608595.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608596</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608596.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608597</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608597.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608598</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608598.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608599</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>608599.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608600 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           value  edge      time  group\n",
       "0       0.190297   0.0       0.0    NaN\n",
       "1       0.164994   0.0       1.0    NaN\n",
       "2       0.177645   0.0       2.0    NaN\n",
       "3       0.194514   0.0       3.0    NaN\n",
       "4       0.160777   0.0       4.0    NaN\n",
       "...          ...   ...       ...    ...\n",
       "608595  0.000000   0.0  608595.0    NaN\n",
       "608596  0.000000   0.0  608596.0    NaN\n",
       "608597  0.000000   0.0  608597.0    NaN\n",
       "608598  0.000000   0.0  608598.0    NaN\n",
       "608599  0.000000   0.0  608599.0    NaN\n",
       "\n",
       "[608600 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# #! Сначала склеить разметку, а потом уже определить разрывы и разметить цифрой 2 начала предупреждающих участков\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "filePath = filedialog.askopenfilename(filetypes=[(\"Parquet Files\", \"*.parquet\")])\n",
    "if filePath:\n",
    "    print(\"Выбранный файл:\", filePath)\n",
    "    fileName, fileExtension = os.path.splitext(filePath)\n",
    "    if fileExtension == '.parquet':\n",
    "        dfOriginal = pd.read_parquet(filePath)\n",
    "        df = dfOriginal.copy()\n",
    "    else:\n",
    "        print(\"Выбран неверный файл\")\n",
    "        exit()\n",
    "else:\n",
    "    print(\"Выбор файла отменен.\")\n",
    "    exit()\n",
    "\n",
    "df['value'] = (df['value'] - df['value'].min()) / (df['value'].max() - df['value'].min())       # Нормализация данных\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondEcgDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.segmentStarts = df.index[df['edge'] > 0].tolist()\n",
    "        self.startsCount = len(self.segmentStarts)\n",
    "\n",
    "        self.cachedTensors = []\n",
    "        self.cachedLabels = []\n",
    "        self._get_segment_tensor()\n",
    "        self.segmentsCount = len(self.cachedTensors)\n",
    "    \n",
    "    def _get_segment_tensor(self):\n",
    "        for index in range(self.startsCount):\n",
    "            result = 0\n",
    "            if index + 1 < len(self.segmentStarts):\n",
    "                if self.df.at[self.segmentStarts[index], 'edge'] != 2:      # По сути условие, что длина отрезка меньше 200\n",
    "                    segment = self.df.iloc[self.segmentStarts[index]:self.segmentStarts[index+1]]\n",
    "                    result = 1\n",
    "            elif self.df.at[self.segmentStarts[index], 'edge'] != 2:        #! Возможно это нужно будет убрать, т.к. тут в выборку может попасть отрезок длиной больше 200\n",
    "                segment = self.df.iloc[self.segmentStarts[index]:]\n",
    "                result = 1\n",
    "\n",
    "            if result:\n",
    "                segmentValues = segment['value']\n",
    "                labelValue = ((segment.iloc[0]['edge'] - 2) if segment.iloc[0]['edge'] > 2.0 else 0)\n",
    "                segmentTensor = torch.tensor(segmentValues.to_numpy(), dtype=torch.float).unsqueeze(1)\n",
    "                # print(segmentTensor.shape)\n",
    "                labelTensor = torch.tensor(labelValue, dtype=torch.float).unsqueeze(0)\n",
    "                # print(labelTensor.shape)\n",
    "                padding = 200 - len(segmentTensor)\n",
    "                if padding > 0:\n",
    "                    segmentTensor = torch.cat((segmentTensor, torch.zeros(padding, 1)), dim=0)\n",
    "                elif padding < 0:\n",
    "                    print('В данные зашёл сегмент неверной длины')\n",
    "                    exit()\n",
    "                self.cachedTensors.append(segmentTensor)\n",
    "                self.cachedLabels.append(labelTensor)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.segmentsCount\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #segmentTensor = self.cachedTensors[index]\n",
    "        #return segmentTensor\n",
    "        return self.cachedTensors[index], self.cachedLabels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5619\n",
      "5057\n",
      "562\n"
     ]
    }
   ],
   "source": [
    "dataset = SecondEcgDataset(df)\n",
    "print(len(dataset))\n",
    "#! Датасет размешивается и разделяется, однако стоит добавить ещё данных и разделить их вручную для более качественного обучения и валидации\n",
    "trainDataset, valDataset = random_split(dataset, [int(0.9 * len(dataset)), len(dataset) - int(0.9 * len(dataset))])\n",
    "# trainDataset, valDataset, testDataset = random_split(dataset, [int(0.6 * len(dataset)), int(0.2 * len(dataset)), len(dataset) - int(0.6 * len(dataset)) - int(0.2 * len(dataset))])\n",
    "print(len(trainDataset))\n",
    "print(len(valDataset))\n",
    "\n",
    "trainDataloader = torch.utils.data.DataLoader(trainDataset, batch_size=1, shuffle=True)\n",
    "valDataloader = torch.utils.data.DataLoader(valDataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=64, num_layers=1, seq_len=200, bidirectional=True):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        add_size = 2*hidden_size if bidirectional else hidden_size\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.lstm2 = nn.LSTM(\n",
    "            input_size=add_size,\n",
    "            hidden_size=hidden_size//2,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.lstm3 = nn.LSTM(\n",
    "            input_size=add_size//2,\n",
    "            hidden_size=hidden_size//4,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        self.resulter = nn.Linear(seq_len * add_size//4, 1)\n",
    "        # self.resulter1 = nn.Linear(seq_len * add_size//4, (seq_len * add_size//4)//10)\n",
    "        # self.resulter2 = nn.Linear((seq_len * add_size//4)//10, (seq_len * add_size//4)//100)\n",
    "        # self.resulter3 = nn.Linear((seq_len * add_size//4)//100, 1)\n",
    "\n",
    "        # Sigmoid\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h1 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "        # c1 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # h2 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size//2).to(x.device)\n",
    "        # c2 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size//2).to(x.device)\n",
    "\n",
    "        # h3 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size//4).to(x.device)\n",
    "        # c3 = torch.zeros(self.num_layers * self.num_directions, x.size(0), self.hidden_size//4).to(x.device)\n",
    "\n",
    "        # Encoder\n",
    "        output, _ = self.lstm1(x)#, (h1, c1))       #Output shape: [1, 200, 128]   [batch_size, seq_len, num_directions * hidden_size]\n",
    "        output, _ = self.lstm2(output)#, (h2, c2))\n",
    "        # _, (hidden, _) = self.lstm3(output)#, (h0, c0))\n",
    "        output, _ = self.lstm3(output)#, (h3, c3))\n",
    "        # print(torch.cat([hidden[-2], hidden[-1]], dim=1).shape)\n",
    "\n",
    "        # Resulter\n",
    "        # output = self.resulter(torch.cat([hidden[-2], hidden[-1]], dim=1))      #! Надо попробовать использовать не Hidden, а Output\n",
    "        batch_size, seq_len, _ = output.size()\n",
    "        output = output.reshape(batch_size, -1)\n",
    "        output = self.resulter(output)      #! Надо попробовать использовать не Hidden, а Output\n",
    "        # output = self.resulter2(output)\n",
    "        # output = self.resulter3(output)\n",
    "        # Надо, чтобы он принимал 32*200 значений\n",
    "        # print(output.shape)\n",
    "\n",
    "        # Sigmoid\n",
    "        output = self.sigmoid(output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAutoencoder(\n",
      "  (lstm1): LSTM(1, 1024, batch_first=True, bidirectional=True)\n",
      "  (lstm2): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
      "  (lstm3): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
      "  (resulter): Linear(in_features=102400, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Параметры для создания модели\n",
    "input_size = 1 # Так как ЭКГ - одномерный сигнал\n",
    "hidden_size = 1024 # Можно изменить в зависимости от сложности задачи и размера данных\n",
    "num_layers = 1 # Можно изменить в зависимости от сложности задачи\n",
    "\n",
    "# Создание экземпляра модели\n",
    "device = (\"cuda\")\n",
    "# model = LSTMAutoencoder(input_size, hidden_size, num_layers).to(device)\n",
    "model = LSTMAutoencoder(input_size, hidden_size, num_layers, 200, True).to(device)\n",
    "print(model)\n",
    "\n",
    "def train(trainDataloader, valDataloader, model, criterion, optimizer):\n",
    "    allTargets = []\n",
    "    allOutputs = []\n",
    "\n",
    "    # Обучение модели\n",
    "    model.train()\n",
    "    trainPbar = tqdm(trainDataloader, desc=\"Training\")\n",
    "    for inputs, targets in trainPbar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        allTargets.append(targets.cpu().detach().squeeze().numpy())\n",
    "        allOutputs.append(outputs.cpu().detach().squeeze().numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainPbar.set_postfix({'Loss': f'{loss.item()}'})\n",
    "    allTargets = np.array(allTargets)\n",
    "    allOutputs = np.array(allOutputs)\n",
    "    # allTargets[allTargets == 0] = 0\n",
    "    # allTargets[allTargets == 0.5] = 0\n",
    "    # allTargets[allTargets == 1] = 1\n",
    "    # allOutputs[allOutputs == 0] = 0\n",
    "    # allOutputs[allOutputs == 0.5] = 0\n",
    "    # allOutputs[allOutputs == 1] = 1\n",
    "    allTargets = np.round(allTargets).astype(int)\n",
    "    allOutputs = np.round(allOutputs).astype(int)\n",
    "    np.set_printoptions(threshold=np.inf)\n",
    "    f1 = f1_score(allTargets, allOutputs)\n",
    "    print(f\"F1: {f1}\")\n",
    "\n",
    "    f1 = 0\n",
    "    allTargets = []\n",
    "    allOutputs = []\n",
    "    # Валидация модели\n",
    "    model.eval() # Переводим модель в режим валидации\n",
    "    valPbar = tqdm(valDataloader, desc=\"Validating\")\n",
    "    with torch.no_grad(): # Отключаем вычисление градиентов\n",
    "        for inputs, targets in valPbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            valLoss = criterion(outputs, targets)\n",
    "            allTargets.append(targets.cpu().detach().squeeze().numpy())\n",
    "            allOutputs.append(outputs.cpu().detach().squeeze().numpy())\n",
    "            valPbar.set_postfix({'Val_Loss': f'{valLoss.item()}'})\n",
    "        allTargets = np.array(allTargets)\n",
    "        allOutputs = np.array(allOutputs)\n",
    "        # allTargets[allTargets == 0] = 0\n",
    "        # allTargets[allTargets == 0.5] = 0\n",
    "        # allTargets[allTargets == 1] = 1\n",
    "        # allOutputs[allOutputs == 0] = 0\n",
    "        # allOutputs[allOutputs == 0.5] = 0\n",
    "        # allOutputs[allOutputs == 1] = 1\n",
    "        allTargets = np.round(allTargets).astype(int)\n",
    "        allOutputs = np.round(allOutputs).astype(int)\n",
    "        f1 = f1_score(allTargets, allOutputs)\n",
    "        print(f\"F1: {f1}\")\n",
    "    \n",
    "    trainPbar.close()\n",
    "    valPbar.close()\n",
    "\n",
    "    return loss.item(), valLoss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5057/5057 [06:30<00:00, 12.95it/s, Loss=0.0362028032541275]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5032258064516129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 562/562 [00:19<00:00, 28.86it/s, Val_Loss=0.01543779019266367] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.47058823529411764\n",
      "Epoch: 1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5057/5057 [06:40<00:00, 12.63it/s, Loss=0.010068332776427269] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.4906832298136646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 562/562 [00:19<00:00, 29.09it/s, Val_Loss=0.01650683395564556]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5882352941176471\n",
      "Epoch: 2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5057/5057 [06:33<00:00, 12.86it/s, Loss=0.01540423184633255]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5045592705167173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 562/562 [00:19<00:00, 29.14it/s, Val_Loss=0.01663736067712307]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.48484848484848486\n",
      "Epoch: 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5057/5057 [06:32<00:00, 12.88it/s, Loss=0.021070268005132675] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5285285285285285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 562/562 [00:19<00:00, 29.19it/s, Val_Loss=0.021188320592045784]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5882352941176471\n",
      "Epoch: 4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 5057/5057 [06:33<00:00, 12.86it/s, Loss=0.02517802082002163]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.525679758308157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 562/562 [00:19<00:00, 29.17it/s, Val_Loss=0.02135414630174637] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.6285714285714286\n",
      "---------------------------------------------------------------------------------------------\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n",
    "\n",
    "ll, vl = 0, 0\n",
    "for epoch in range(5):\n",
    "    print(f'Epoch: {epoch} ')\n",
    "    ll, vl = train(trainDataloader, valDataloader, model, criterion, optimizer)\n",
    "\n",
    "print('---------------------------------------------------------------------------------------------')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.2790231704711914\n",
      "ValLoss:  0.2800871431827545\n"
     ]
    }
   ],
   "source": [
    "print('Loss: ', ll)\n",
    "print('ValLoss: ', vl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "torch.save(model.state_dict(), '../.workspace/Models/SecondStageModel.53.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
